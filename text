import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import fitz  # PyMuPDF
import string
import random
import ssl
import numpy as np
from gensim.models import KeyedVectors

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Download NLTK data files (only need to run once)
nltk.download('punkt')

pdf_path = '/Users/rajesh/Desktop/Pdfs/genai-principles.pdf'  # Path to your PDF file


def extract_text_from_pdf(pdf_path):
    text = ""
    with fitz.open(pdf_path) as pdf_document:
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    return text


text = extract_text_from_pdf(pdf_path)

raw = text.lower()  # Converts to lowercase
sent_tokens = nltk.sent_tokenize(raw)  # Converts to list of sentences


# Load GloVe embeddings
def load_glove_embeddings(file_path):
    print(f"Loading GloVe embeddings from {file_path}...")
    glove_model = KeyedVectors.load_word2vec_format(file_path, binary=False, no_header=True)
    return glove_model


# Specify the path to your GloVe embeddings file
glove_path = '/Users/rajesh/Downloads/glove.6B.50d.txt'  # Make sure to set the correct path
glove_model = load_glove_embeddings(glove_path)


def get_word_vector(word, glove_model):
    try:
        return glove_model[word]
    except KeyError:
        # Return a zero vector if the word is not in the vocabulary
        return np.zeros(glove_model.vector_size)


def get_sentence_vector(sentence, glove_model):
    words = word_tokenize(sentence)
    word_vectors = [get_word_vector(word, glove_model) for word in words]
    return np.mean(word_vectors, axis=0)


def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)


GREETING_INPUTS = ("hello", "hi", "greetings", "sup", "what's up", "hey")
GREETING_RESPONSES = ["hi", "hey", "*nods*", "hi there", "hello", "I am glad! You are talking to me"]


def greeting(sentence):
    for word in sentence.split():
        if word.lower() in GREETING_INPUTS:
            return random.choice(GREETING_RESPONSES)


def response(user_response):
    if not sent_tokens:
        return "I don't have enough information to answer."

    user_vector = get_sentence_vector(user_response, glove_model)

    best_match_idx = -1
    best_similarity = -1

    for i, sentence in enumerate(sent_tokens):
        sentence_vector = get_sentence_vector(sentence, glove_model)
        similarity = cosine_similarity(user_vector, sentence_vector)
        if similarity > best_similarity:
            best_similarity = similarity
            best_match_idx = i

    if best_similarity > 0.5:  # Adjust threshold as needed
        return sent_tokens[best_match_idx]
    else:
        return "I am sorry! I don't understand you."


flag = True
print("ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!")

while flag:
    user_response = input().lower()
    if user_response != 'bye':
        if user_response in ('thanks', 'thank you'):
            flag = False
            print("ROBO: You are welcome..")
        else:
            if greeting(user_response) is not None:
                print("ROBO: " + greeting(user_response))
            else:
                print("ROBO: " + response(user_response))
    else:
        flag = False
        print("ROBO: Bye! Take care..")

