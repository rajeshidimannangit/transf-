import pdfplumber
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from gensim.models import KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text


# Function to preprocess text
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalnum()]
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return tokens


# Function to get average vector of tokens
def get_average_vector(tokens, model):
    vectors = [model[word] for word in tokens if word in model]
    if vectors:
        return sum(vectors) / len(vectors)
    else:
        return None


# Load pre-trained Word2Vec model
def load_word2vec_model(model_path):
    return KeyedVectors.load_word2vec_format(model_path, binary=True)


# Function to compute cosine similarity
def compute_similarity(vector1, vector2):
    if vector1 is not None and vector2 is not None:
        return cosine_similarity([vector1], [vector2])[0][0]
    else:
        return 0


# Function to split text into sections
def split_text_into_sections(text, section_length=1000):
    return [text[i:i + section_length] for i in range(0, len(text), section_length)]


# Main function to perform semantic search
def semantic_search(pdf_path, query, model_path):
    # Extract and preprocess PDF text
    pdf_text = extract_text_from_pdf(pdf_path)
    pdf_tokens = preprocess_text(pdf_text)

    # Load Word2Vec model
    model = load_word2vec_model(model_path)

    # Vectorize the entire PDF text
    pdf_text_vector = get_average_vector(pdf_tokens, model)

    # Vectorize the user query
    query_tokens = preprocess_text(query)
    query_vector = get_average_vector(query_tokens, model)

    # Split PDF text into sections and vectorize each section
    sections = split_text_into_sections(pdf_text)
    section_vectors = [get_average_vector(preprocess_text(section), model) for section in sections]

    # Compute similarity scores for each section
    section_similarities = [compute_similarity(query_vector, section_vector) for section_vector in section_vectors]

    # Find the most relevant section
    most_relevant_index = np.argmax(section_similarities)
    most_relevant_section = sections[most_relevant_index]

    return most_relevant_section


# Example usage
pdf_path = '/Users/rajesh/Desktop/Pdfs/genai-principles.pdf'  # Replace with your PDF path
user_query = 'tell me about the conclusion'  # Replace with your query
model_path = '/Users/rajesh/Downloads/GoogleNews-vectors-negative300.bin'  # Replace with your Word2Vec model path

# Perform semantic search
result = semantic_search(pdf_path, user_query, model_path)
print("Most relevant section based on your query:")
print(result)
